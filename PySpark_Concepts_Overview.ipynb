{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLnbvp2zmYMj",
        "outputId": "605eabd3-b1b1-4ea2-b5e3-f750fec5f8ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=4b374a7903c25f15d97f123bd64666d760ce3f10c974c002266176d58eec6973\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RDD Operations Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create an RDD from a list\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Example 1: Using map to square each element\n",
        "squared_rdd = rdd.map(lambda x: x * x)\n",
        "print(\"Squared elements:\", squared_rdd.collect())\n",
        "\n",
        "# Example 2: Using filter to get even numbers\n",
        "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "print(\"Even elements:\", even_rdd.collect())\n",
        "\n",
        "# Example 3: Using flatMap to create a new sequence\n",
        "flattened_rdd = rdd.flatMap(lambda x: (x, x * 3))\n",
        "print(\"Flattened elements:\", flattened_rdd.collect())\n",
        "\n",
        "# Example 4: Using count to get the number of elements\n",
        "count = rdd.count()\n",
        "print(\"Number of elements:\", count)\n",
        "\n",
        "# Example 5: Using reduce to sum elements\n",
        "sum_of_elements = rdd.reduce(lambda a, b: a + b)\n",
        "print(\"Sum of elements:\", sum_of_elements)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYecdMqtmlqK",
        "outputId": "e519660b-51c8-4bb2-af51-990d6e3a0da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared elements: [1, 4, 9, 16, 25]\n",
            "Even elements: [2, 4]\n",
            "Flattened elements: [1, 3, 2, 6, 3, 9, 4, 12, 5, 15]\n",
            "Number of elements: 5\n",
            "Sum of elements: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame Operations Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create DataFrame from list of Rows\n",
        "rdd = spark.sparkContext.parallelize([Row(id=1, name=\"John\"), Row(id=2, name=\"Mike\")])\n",
        "df = spark.createDataFrame(rdd)\n",
        "\n",
        "# Show DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# Select operation\n",
        "print(\"Select 'name' column:\")\n",
        "df.select(\"name\").show()\n",
        "\n",
        "# Filter operation\n",
        "print(\"Filter by 'id > 1':\")\n",
        "df.filter(df.id > 1).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9lYJa6bnNEG",
        "outputId": "dd4491fe-82f6-4c21-e7d0-ae4d3bbd9c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|John|\n",
            "|  2|Mike|\n",
            "+---+----+\n",
            "\n",
            "Select 'name' column:\n",
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|John|\n",
            "|Mike|\n",
            "+----+\n",
            "\n",
            "Filter by 'id > 1':\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  2|Mike|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Image Audio Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Add a path to a file. In this example, we assume a local path, but you can add HDFS paths as well\n",
        "spark.sparkContext.addFile(\"/content/image_cutout2.png\")\n",
        "\n",
        "# Read the image file\n",
        "with open(SparkFiles.get(\"image_cutout2.png\"), \"rb\") as f:\n",
        "    img_data = f.read()\n",
        "\n",
        "# Convert to an image format you can work with (using PIL in this example)\n",
        "image = Image.open(io.BytesIO(img_data))\n"
      ],
      "metadata": {
        "id": "IqMOdsT4n2Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "\n",
        "# Add a path to a file. In this example, we assume a local path, but you can add HDFS paths as well\n",
        "\n",
        "#/tmp/spark-3543400b-ce35-4e08-a562-93597cfab306/userFiles-a0f4eb7b-de8e-441a-bc44-9d24e90eb12c/srk2.mp3\n",
        "spark.sparkContext.addFile(\"/content/srk2.mp3\")\n",
        "\n",
        "# Read the audio file\n",
        "with open(SparkFiles.get(\"srk2.mp3\"), \"rb\") as f:\n",
        "    audio_data = f.read()\n",
        "\n",
        "# Convert to an audio format you can work with (using soundfile in this example)\n",
        "audio, samplerate = sf.read(io.BytesIO(audio_data))\n"
      ],
      "metadata": {
        "id": "bf0jN3l6o2A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Parallel Image Processing\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# List of image file paths (adjust these paths to your actual file locations)\n",
        "image_paths = [\"/content/image_cutout2.png\", \"/content/image_cutout2.png\"]\n",
        "\n",
        "# Create an RDD from the list of image paths\n",
        "image_paths_rdd = spark.sparkContext.parallelize(image_paths)\n",
        "\n",
        "# Function to read and process image\n",
        "def read_and_process_image(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        img_data = f.read()\n",
        "    image = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "    # Convert to 'RGB' if the image is 'RGBA'\n",
        "    if image.mode == 'RGBA':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    # Perform some operation on the image (e.g., resizing)\n",
        "    image_resized = image.resize((100, 100))\n",
        "\n",
        "    # Convert image to byte array (if needed for further processing)\n",
        "    img_byte_arr = io.BytesIO()\n",
        "    image_resized.save(img_byte_arr, format='JPEG')\n",
        "    img_byte_arr = img_byte_arr.getvalue()\n",
        "\n",
        "    return img_byte_arr\n",
        "\n",
        "# Use map to read and process images in parallel\n",
        "processed_images_rdd = image_paths_rdd.map(read_and_process_image)\n",
        "\n",
        "# Collect results (use with caution; not recommended on large datasets)\n",
        "processed_images = processed_images_rdd.collect()\n",
        "\n",
        "# At this point, processed_images is a list of byte arrays representing the processed images\n"
      ],
      "metadata": {
        "id": "7N2HLYWGpUSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Parallel Image Processing\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# List of image file paths (adjust these paths to your actual file locations)\n",
        "image_paths = [\"/content/image_cutout2.png\", \"/content/image_cutout2.png\"]\n",
        "\n",
        "# Create an RDD from the list of image paths\n",
        "image_paths_rdd = spark.sparkContext.parallelize(image_paths)\n",
        "\n",
        "# Function to read and process image\n",
        "def read_and_process_image(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        img_data = f.read()\n",
        "    image = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "    # Convert to 'RGB' if the image is 'RGBA'\n",
        "    if image.mode == 'RGBA':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    # Perform some operation on the image (e.g., resizing)\n",
        "    image_resized = image.resize((100, 100))\n",
        "\n",
        "    # Convert image to byte array (if needed for further processing)\n",
        "    img_byte_arr = io.BytesIO()\n",
        "    image_resized.save(img_byte_arr, format='JPEG')\n",
        "    img_byte_arr = img_byte_arr.getvalue()\n",
        "\n",
        "    # Logic to save the image to disk (replace with your own logic if needed)\n",
        "    output_path = \"./processed_\" + os.path.basename(file_path)\n",
        "    with open(output_path, 'wb') as f_out:\n",
        "        f_out.write(img_byte_arr)\n",
        "\n",
        "# Use foreach to process and save images in parallel\n",
        "image_paths_rdd.foreach(read_and_process_image)\n"
      ],
      "metadata": {
        "id": "Gg0rF73RsH91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import soundfile as sf\n",
        "import io\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Parallel Audio Processing using Collect\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# List of audio file paths (adjust these paths to your actual file locations)\n",
        "audio_paths = [\"/content/srk2.mp3\", \"/content/srk2.mp3\"]\n",
        "\n",
        "# Create an RDD from the list of audio paths\n",
        "audio_paths_rdd = spark.sparkContext.parallelize(audio_paths)\n",
        "\n",
        "# Function to read and process audio\n",
        "def read_and_process_audio(file_path):\n",
        "    audio, samplerate = sf.read(file_path)\n",
        "    # Perform some operation on the audio data (e.g., taking the first 10 samples)\n",
        "    audio_processed = audio[:10]\n",
        "    return audio_processed\n",
        "\n",
        "# Use map to read and process audio in parallel\n",
        "processed_audio_rdd = audio_paths_rdd.map(read_and_process_audio)\n",
        "\n",
        "# Collect results (use with caution; not recommended on large datasets)\n",
        "processed_audio = processed_audio_rdd.collect()\n",
        "\n",
        "# At this point, processed_audio is a list of processed audio data arrays\n"
      ],
      "metadata": {
        "id": "3NsM_dDvsaRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import soundfile as sf\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Parallel Audio Processing using Foreach\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# List of audio file paths (adjust these paths to your actual file locations)\n",
        "audio_paths = [\"/content/srk2.mp3\", \"/content/srk2.mp3\"]\n",
        "\n",
        "# Create an RDD from the list of audio paths\n",
        "audio_paths_rdd = spark.sparkContext.parallelize(audio_paths)\n",
        "\n",
        "# Function to read and process audio\n",
        "def read_and_process_audio(file_path):\n",
        "    audio, samplerate = sf.read(file_path)\n",
        "    # Perform some operation on the audio data (e.g., taking the first 10 samples)\n",
        "    audio_processed = audio[:10]\n",
        "\n",
        "    # Logic to save the processed audio to disk (replace with your own logic if needed)\n",
        "    output_path = \"./processed_\" + os.path.basename(file_path)\n",
        "    sf.write(output_path, audio_processed, samplerate)\n",
        "\n",
        "# Use foreach to process and save audio in parallel\n",
        "audio_paths_rdd.foreach(read_and_process_audio)\n"
      ],
      "metadata": {
        "id": "FLnV-37csx4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark SQL Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Catherine\", 3)]\n",
        "columns = [\"Name\", \"ID\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Create a Spark SQL table from DataFrame\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Run SQL queries\n",
        "result = spark.sql(\"SELECT * FROM people WHERE ID > 1\")\n",
        "\n",
        "# Create DataFrames\n",
        "df1 = spark.createDataFrame([(1, \"John\"), (2, \"Mike\"), (3, \"Sara\")], [\"ID\", \"Name\"])\n",
        "df2 = spark.createDataFrame([(1, \"Math\"), (2, \"History\"), (3, \"Physics\")], [\"ID\", \"Subject\"])\n",
        "\n",
        "# Create Spark SQL tables\n",
        "df1.createOrReplaceTempView(\"students\")\n",
        "df2.createOrReplaceTempView(\"subjects\")\n",
        "\n",
        "# SQL Query: Join students with subjects on ID\n",
        "result_sql = spark.sql(\"SELECT students.Name, subjects.Subject FROM students JOIN subjects ON students.ID = subjects.ID\")\n",
        "result_sql.show()\n",
        "\n",
        "# DataFrame API: Equivalent join operation\n",
        "result_df = df1.join(df2, \"ID\").select(\"Name\", \"Subject\")\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlrAgvlctEvm",
        "outputId": "a79144ed-f676-462c-9c29-7692eff0e6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "|Name|Subject|\n",
            "+----+-------+\n",
            "|John|   Math|\n",
            "|Mike|History|\n",
            "|Sara|Physics|\n",
            "+----+-------+\n",
            "\n",
            "+----+-------+\n",
            "|Name|Subject|\n",
            "+----+-------+\n",
            "|John|   Math|\n",
            "|Mike|History|\n",
            "|Sara|Physics|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark SQL and DataFrame API Examples\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create DataFrames\n",
        "df1 = spark.createDataFrame([(1, \"John\", 28), (2, \"Mike\", 22), (3, \"Sara\", 25)], [\"ID\", \"Name\", \"Age\"])\n",
        "df2 = spark.createDataFrame([(1, \"Math\"), (2, \"History\"), (3, \"Physics\")], [\"ID\", \"Subject\"])\n",
        "\n",
        "# Create Spark SQL tables\n",
        "df1.createOrReplaceTempView(\"students\")\n",
        "df2.createOrReplaceTempView(\"subjects\")\n",
        "\n",
        "# ----------------------\n",
        "# Spark SQL Operations\n",
        "# ----------------------\n",
        "\n",
        "# a. Select All Records\n",
        "result1 = spark.sql(\"SELECT * FROM students\")\n",
        "result1.show()\n",
        "\n",
        "# b. Where Clause\n",
        "result2 = spark.sql(\"SELECT * FROM students WHERE Age > 25\")\n",
        "result2.show()\n",
        "\n",
        "# c. Join Tables\n",
        "result3 = spark.sql(\"SELECT students.Name, subjects.Subject FROM students JOIN subjects ON students.ID = subjects.ID\")\n",
        "result3.show()\n",
        "\n",
        "# ---------------------------\n",
        "# DataFrame API Operations\n",
        "# ---------------------------\n",
        "\n",
        "# a. Select Columns\n",
        "df1.select(\"Name\").show()\n",
        "\n",
        "# b. Filter Records\n",
        "df1.filter(df1.Age > 25).show()\n",
        "\n",
        "# c. Join DataFrames\n",
        "joined_df = df1.join(df2, \"ID\").select(\"Name\", \"Subject\")\n",
        "joined_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGorI5G-tkVW",
        "outputId": "98d1f419-1a69-4568-c32e-24f521801582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+\n",
            "| ID|Name|Age|\n",
            "+---+----+---+\n",
            "|  1|John| 28|\n",
            "|  2|Mike| 22|\n",
            "|  3|Sara| 25|\n",
            "+---+----+---+\n",
            "\n",
            "+---+----+---+\n",
            "| ID|Name|Age|\n",
            "+---+----+---+\n",
            "|  1|John| 28|\n",
            "+---+----+---+\n",
            "\n",
            "+----+-------+\n",
            "|Name|Subject|\n",
            "+----+-------+\n",
            "|John|   Math|\n",
            "|Mike|History|\n",
            "|Sara|Physics|\n",
            "+----+-------+\n",
            "\n",
            "+----+\n",
            "|Name|\n",
            "+----+\n",
            "|John|\n",
            "|Mike|\n",
            "|Sara|\n",
            "+----+\n",
            "\n",
            "+---+----+---+\n",
            "| ID|Name|Age|\n",
            "+---+----+---+\n",
            "|  1|John| 28|\n",
            "+---+----+---+\n",
            "\n",
            "+----+-------+\n",
            "|Name|Subject|\n",
            "+----+-------+\n",
            "|John|   Math|\n",
            "|Mike|History|\n",
            "|Sara|Physics|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark SQL and DataFrame API Parallel Execution Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create DataFrames\n",
        "df1 = spark.createDataFrame([(1, \"John\", 28), (2, \"Mike\", 22), (3, \"Sara\", 25)], [\"ID\", \"Name\", \"Age\"])\n",
        "df2 = spark.createDataFrame([(1, \"Math\"), (2, \"History\"), (3, \"Physics\")], [\"ID\", \"Subject\"])\n",
        "\n",
        "# Create Spark SQL tables\n",
        "df1.createOrReplaceTempView(\"students\")\n",
        "df2.createOrReplaceTempView(\"subjects\")\n",
        "\n",
        "# -------------- Parallel Execution Example --------------\n",
        "# These operations are inherently parallelized by Spark\n",
        "\n",
        "# Spark SQL Join Query\n",
        "result_sql = spark.sql(\"SELECT students.Name, subjects.Subject FROM students JOIN subjects ON students.ID = subjects.ID\")\n",
        "\n",
        "# DataFrame API Join Operation\n",
        "result_df = df1.join(df2, \"ID\").select(\"Name\", \"Subject\")\n",
        "\n",
        "# Both operations are automatically parallelized by Spark\n",
        "# --------------------------------------------------------\n",
        "\n",
        "# Custom partitioning: Repartition df1 into 50 partitions\n",
        "df1_repartitioned = df1.repartition(50)\n",
        "\n",
        "# Note: The 'show()' command is used for demonstration.\n",
        "# In a real-world application, you would typically perform further transformations or actions based on your needs.\n",
        "result_sql.show()\n",
        "result_df.show()\n",
        "\n",
        "# Output: The joined DataFrames or tables, executed in parallel\n"
      ],
      "metadata": {
        "id": "NHRQ41K1uo7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Real-life Example of PySpark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create Employee Data DataFrame\n",
        "employee_data = spark.createDataFrame([\n",
        "    (1, \"Alice\", \"HR\"),\n",
        "    (2, \"Bob\", \"Engineering\"),\n",
        "    (3, \"Catherine\", \"Finance\"),\n",
        "], [\"ID\", \"Name\", \"Department\"])\n",
        "\n",
        "# Create Salary Data DataFrame\n",
        "salary_data = spark.createDataFrame([\n",
        "    (1, 50000),\n",
        "    (2, 120000),\n",
        "    (3, 90000),\n",
        "], [\"ID\", \"Annual_Salary\"])\n",
        "\n",
        "# Repartition employee_data for optimized performance\n",
        "employee_data_repartitioned = employee_data.repartition(50)\n",
        "\n",
        "# Join Employee Data with Salary Data on ID\n",
        "joined_data = employee_data_repartitioned.join(salary_data, \"ID\")\n",
        "\n",
        "# Filter out employees with an Annual Salary less than 60000\n",
        "filtered_data = joined_data.filter(joined_data.Annual_Salary >= 60000)\n",
        "\n",
        "# Show the final DataFrame\n",
        "filtered_data.show()\n",
        "\n",
        "# Note: In a real-world scenario, you may also save this filtered data to a database or another data storage system.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rF9YIOKv8t4",
        "outputId": "f6c69b96-e777-4ddd-acf3-2be1b6a5930c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-----------+-------------+\n",
            "| ID|     Name| Department|Annual_Salary|\n",
            "+---+---------+-----------+-------------+\n",
            "|  2|      Bob|Engineering|       120000|\n",
            "|  3|Catherine|    Finance|        90000|\n",
            "+---+---------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Initialize existing or new Spark Context\n",
        "sc = SparkContext.getOrCreate()\n",
        "if sc is None:\n",
        "    sc = SparkContext(appName=\"WordCountApp\")\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Initialize Streaming Context with a batch interval of 1 second\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "\n",
        "# Create a DStream that connects to a TCP source on hostname:port\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "print(lines)\n",
        "\n",
        "# Split lines into words and count each word in each batch\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "wordCounts = words.countByValue()\n",
        "\n",
        "# Print the count of each word\n",
        "wordCounts.pprint()\n",
        "\n",
        "# Start the computation\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "lIHXNRi1xcn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nc -lk 9999 #To send message in the console for streaming"
      ],
      "metadata": {
        "id": "ABNYMVzDxebQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Initialize existing or new Spark Context\n",
        "sc = SparkContext.getOrCreate()\n",
        "if sc is None:\n",
        "    sc = SparkContext(appName=\"RealTimeWordCount\")\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Initialize Streaming Context with a batch interval of 10 seconds\n",
        "ssc = StreamingContext(sc, 10)\n",
        "\n",
        "# Create a DStream that connects to a TCP source on hostname:port\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Split lines into words\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Count each word in each batch within a window of 30 seconds, sliding every 10 seconds\n",
        "windowedWordCounts = words.countByValueAndWindow(windowDuration=30, slideDuration=10)\n",
        "\n",
        "# Print the count of each word\n",
        "windowedWordCounts.pprint()\n",
        "\n",
        "# Start the computation\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "fsIWGIVZyZZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "df_pyspark=spark.read.csv('test.csv',header=True,inferSchema=True)\n",
        "imputer = Imputer(\n",
        "    inputCols=['age', 'Experience', 'Salary'],\n",
        "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
        "    ).setStrategy(\"median\")\n",
        "# Add imputation cols to df\n",
        "imputer.fit(df_pyspark).transform(df_pyspark).show()"
      ],
      "metadata": {
        "id": "y9xDGWrIn6RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"LogisticRegressionPipeline\").getOrCreate()\n",
        "\n",
        "# Create DataFrame with training data\n",
        "training_data = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
        "    (0.0, Vectors.dense([2.0, 1.1, -1.0])),\n",
        "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
        "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))\n",
        "], [\"label\", \"features\"])\n",
        "\n",
        "# Initialize Standard Scaler\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "\n",
        "# Initialize Logistic Regression Model\n",
        "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
        "\n",
        "# Build the pipeline\n",
        "pipeline = Pipeline(stages=[scaler, lr])\n",
        "\n",
        "# Initialize Grid for Hyperparameter Tuning\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.maxIter, [10, 50, 100]) \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.05, 0.1]) \\\n",
        "    .build()\n",
        "\n",
        "# Initialize CrossValidator\n",
        "cross_val = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=param_grid,\n",
        "    evaluator=BinaryClassificationEvaluator(),\n",
        "    numFolds=3\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "cv_model = cross_val.fit(training_data)\n",
        "\n",
        "# Get the best model\n",
        "best_model = cv_model.bestModel\n",
        "\n",
        "# Extract the stages of the best model\n",
        "scaler_stage = best_model.stages[0]\n",
        "lr_stage = best_model.stages[1]\n",
        "\n",
        "# Show hyperparameters of the best model\n",
        "print(f\"Best MaxIter: {lr_stage._java_obj.getMaxIter()}\")\n",
        "print(f\"Best RegParam: {lr_stage._java_obj.getRegParam()}\")\n"
      ],
      "metadata": {
        "id": "YAGkAhaxziLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"SimplifiedLogisticRegressionPipeline\").getOrCreate()\n",
        "\n",
        "# Create DataFrame with training data\n",
        "training_data = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
        "    (0.0, Vectors.dense([2.0, 1.1, -1.0])),\n",
        "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
        "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))\n",
        "], [\"label\", \"features\"])\n",
        "\n",
        "# Initialize Standard Scaler\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "\n",
        "# Initialize Logistic Regression Model\n",
        "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
        "\n",
        "# Build the pipeline\n",
        "pipeline = Pipeline(stages=[scaler, lr])\n",
        "\n",
        "# Fit the model\n",
        "model = pipeline.fit(training_data)\n",
        "\n",
        "# Extract Logistic Regression from the fitted model\n",
        "lr_model = model.stages[-1]\n",
        "\n",
        "# Show hyperparameters of the model\n",
        "print(f\"MaxIter: {lr_model._java_obj.getMaxIter()}\")\n",
        "print(f\"RegParam: {lr_model._java_obj.getRegParam()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyaaAwu0zkGq",
        "outputId": "33319fcb-0782-4dd1-ab2b-51a52aa30ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2023-09-06 09:24:10\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2023-09-06 09:24:11\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2023-09-06 09:24:12\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2023-09-06 09:24:13\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2023-09-06 09:24:14\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2023-09-06 09:24:15\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2023-09-06 09:24:16\n",
            "-------------------------------------------\n",
            "\n",
            "MaxIter: 100\n",
            "RegParam: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphframes\n",
        "from pyspark.sql import SparkSession\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"GraphFramesExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame for vertices\n",
        "v = spark.createDataFrame([\n",
        "  (\"1\", \"Alice\"),\n",
        "  (\"2\", \"Bob\"),\n",
        "  (\"3\", \"Charlie\"),\n",
        "  (\"4\", \"David\")\n",
        "], [\"id\", \"name\"])\n",
        "\n",
        "# Create a DataFrame for edges\n",
        "e = spark.createDataFrame([\n",
        "  (\"1\", \"2\", \"friend\"),\n",
        "  (\"2\", \"3\", \"follows\"),\n",
        "  (\"3\", \"4\", \"friend\"),\n",
        "  (\"4\", \"1\", \"follows\")\n",
        "], [\"src\", \"dst\", \"relationship\"])\n",
        "\n",
        "# Create a GraphFrame\n",
        "g = GraphFrame(v, e)\n",
        "\n",
        "# Run PageRank algorithm, and show results.\n",
        "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
        "results.vertices.select(\"id\", \"pagerank\").show()\n"
      ],
      "metadata": {
        "id": "ckdM_4hL1lps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the existing SparkContext\n",
        "sc.stop()\n",
        "\n",
        "# Initialize a new SparkContext with specific configurations\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setAppName(\"PerformanceTuningExample\").set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 100000))\n",
        "\n",
        "# Cache the RDD\n",
        "rdd.persist()\n",
        "\n",
        "# Perform some actions\n",
        "result1 = rdd.count()\n",
        "result2 = rdd.sum()\n",
        "\n",
        "print(result1,result2)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAWd1M-41oWr",
        "outputId": "27d3436d-39d7-443a-8d10-4fb093cd5fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99999 4999950000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Stop any existing SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc.stop()\n",
        "\n",
        "# Initialize a new SparkContext with specific configurations\n",
        "conf = SparkConf().setAppName(\"PerformanceTuningExample\").set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Initialize Spark Session with custom shuffle partitions and executor memory\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PerformanceTuningExample\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize([(1, 1), (2, 1), (3, 1), (1, 1), (2, 1), (3, 1)])\n",
        "\n",
        "# Using groupByKey (Not recommended due to more shuffling)\n",
        "grouped_result = rdd.groupByKey().mapValues(sum).collect()\n",
        "print(\"Result with groupByKey:\", grouped_result)\n",
        "\n",
        "# Using reduceByKey (Recommended)\n",
        "reduced_result = rdd.reduceByKey(lambda a, b: a + b).collect()\n",
        "print(\"Result with reduceByKey:\", reduced_result)\n",
        "\n",
        "# Stop the SparkContext and SparkSession to free up resources\n",
        "sc.stop()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfZWjkLt3frn",
        "outputId": "8ac85d77-295c-4800-e86a-fcd612f734f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result with groupByKey: [(2, 2), (1, 2), (3, 2)]\n",
            "Result with reduceByKey: [(2, 2), (1, 2), (3, 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit \\\n",
        "  --master yarn \\\n",
        "  --deploy-mode cluster \\\n",
        "  --name MySparkApp \\\n",
        "  --executor-memory 2G \\\n",
        "  --num-executors 3 \\\n",
        "  my_spark_app.py\n",
        "#To deploy on the standalone cluster, a cloud environment, or a Hadoop YARN cluster."
      ],
      "metadata": {
        "id": "ZC8NBIJR3--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Dockerfile\n",
        "# Use a base image with Java\n",
        "FROM openjdk:8-jdk-alpine\n",
        "\n",
        "# Set environment variables for Spark\n",
        "ENV SPARK_VERSION=3.1.2\n",
        "ENV HADOOP_VERSION=3.2\n",
        "\n",
        "# Install Spark\n",
        "RUN apk add --no-cache wget && \\\n",
        "    wget https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n",
        "    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n",
        "    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /spark && \\\n",
        "    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\n",
        "\n",
        "# Add Spark binaries to PATH\n",
        "ENV PATH $PATH:/spark/bin\n",
        "'''\n",
        "#To install Spark using Docker"
      ],
      "metadata": {
        "id": "l48C-PuT4ps4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit \\\n",
        "  --master k8s://<KUBERNETES_CLUSTER_ENDPOINT> \\\n",
        "  --deploy-mode cluster \\\n",
        "  --name my-spark-job \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --conf spark.executor.instances=3 \\\n",
        "  --conf spark.kubernetes.container.image=my-spark-image:latest \\\n",
        "  local:///path/to/spark/examples/jars/spark-examples_2.12-3.1.2.jar\n"
      ],
      "metadata": {
        "id": "KRFAPuTN5H5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}